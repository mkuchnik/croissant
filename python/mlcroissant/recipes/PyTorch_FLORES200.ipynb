{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4763b9",
   "metadata": {},
   "source": [
    "# Using Croissant ü•ê in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a84fb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df08e22",
   "metadata": {},
   "source": [
    "The Croissant format is designed to be portable across machine learning frameworks.\n",
    "In this notebook, we'll show how Croissant can be used within the PyTorch framework.\n",
    "For the example, we'll be using the [FLORES-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md) dataset with a [BERT](https://arxiv.org/abs/1810.04805) model hosted on [Hugging Face transformers](https://huggingface.co/bert-base-multilingual-cased).\n",
    "This notebook fine-tunes the BERT model to classify between the 200+ languages present in FLORES-200, also known as *language identification*.\n",
    "In this notebook, we utilize a minimal DataPipe adapter on top of mlcroissant to help facilitate dataloading.\n",
    "\n",
    "First, let's start by installing some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd14b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install some needed dependencies for the notebook\n",
    "!pip3 install --quiet mlcroissant[git] torch torchvision torchaudio torchdata transformers numpy graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02331e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch_adapter as ta\n",
    "import torchdata\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7052bd",
   "metadata": {},
   "source": [
    "Next, we need to provide the PyTorch adapter a Croissant file to read!\n",
    "We pass the FLORES-200 Croissant file to the class, which we will subsequently use to generate a PyTorch DataPipe for loading data.\n",
    "For convenience, we specify that certain fields should be automatically converted to their corresponding data type.\n",
    "We do this because Croissant implementations can return encoded data (e.g., UTF-8 bytes) for certain fields, but applications may expect decoded data (e.g., text strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cbfa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_factory = ta.LoaderFactory(file=\"../../../datasets/flores-200/metadata.json\")\n",
    "specification = {\n",
    "    \"translation\": ta.LoaderSpecificationDataType.INFER,\n",
    "    \"language\": ta.LoaderSpecificationDataType.INFER,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03576775",
   "metadata": {},
   "source": [
    "Using the adapter factory, we create a DataPipe.\n",
    "Importantly, we create one DataPipe per record set.\n",
    "In this case, this translates to a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6be15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [dataset(FLORES-200)] Property \"https://schema.org/version\" is recommended, but does not exist.\n",
      "WARNING:absl:Found the following 1 warning(s) during the validation:\n",
      "  -  [dataset(FLORES-200)] Property \"https://schema.org/version\" is recommended, but does not exist.\n"
     ]
    }
   ],
   "source": [
    "train_data_pipe = ta_factory.as_datapipe(\n",
    "    record_set=\"language_translations_train_data_with_metadata\",\n",
    "    specification=specification,\n",
    ")\n",
    "test_data_pipe = ta_factory.as_datapipe(\n",
    "    record_set=\"language_translations_test_data_with_metadata\",\n",
    "    specification=specification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c94a9",
   "metadata": {},
   "source": [
    "Finally, we get a DataPipe, which we can use to manipulate data with traditional PyTorch tools!\n",
    "We can observe that these DataPipes are shallow wrappers around the base data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fd5d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"115pt\" height=\"82pt\"\n",
       " viewBox=\"0.00 0.00 115.00 82.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 78)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-78 111,-78 111,4 -4,4\"/>\n",
       "<!-- Mapper&#45;373012000 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Mapper&#45;373012000</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"80.5,-19 26.5,-19 26.5,0 80.5,0 80.5,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\">Mapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;373011901 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>IterableWrapper&#45;373011901</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107,-74 0,-74 0,-55 107,-55 107,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">IterableWrapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;373011901&#45;&gt;Mapper&#45;373012000 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>IterableWrapper&#45;373011901&#45;&gt;Mapper&#45;373012000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-54.75C53.5,-47.8 53.5,-37.85 53.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-29.09 53.5,-19.09 50,-29.09 57,-29.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x163ce8940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdata.datapipes.utils.to_graph(train_data_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186f9ef",
   "metadata": {},
   "source": [
    "## Setting Up The Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac698f",
   "metadata": {},
   "source": [
    "The first thing we have to do to use the data in a classification setting is map each language, which represents the target class, to an integer.\n",
    "We'll quickly iterate the unique language descriptors to build this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae257dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_int = {\n",
    "    y: i for i, y in enumerate(sorted(set(x[\"language\"] for x in train_data_pipe)))\n",
    "}\n",
    "int_to_classes = {i: y for (y, i) in classes_to_int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeafc15",
   "metadata": {},
   "source": [
    "Next, we need to set some hyperparameters that the rest of the code will use.\n",
    "The pretrained model, the sequence dimensions, and performance-related settings are specified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31a2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LENGTH = 10\n",
    "LEARNING_RATE = 1e-5\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 256\n",
    "NUM_TRAIN_WORKERS = 0\n",
    "NUM_TEST_WORKERS = 0\n",
    "NUM_TRAIN_SAMPLES = 203388\n",
    "NUM_TEST_SAMPLES = 206448\n",
    "NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7242166",
   "metadata": {},
   "source": [
    "Next, we have to specify how the tokenization should be done.\n",
    "We'll use the tokenizer that was used with the BERT model.\n",
    "We'll also apply some padding and truncation to the tensors to simplify the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf44da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=1)\n",
    "def get_tokenizer():\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def unpack_row(x):\n",
    "    tokenizer = get_tokenizer()\n",
    "    tokenized = tokenizer(\n",
    "        x[\"translation\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    targets = classes_to_int[x[\"language\"]]\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"].reshape((-1,)),\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"].reshape((-1,)),\n",
    "        \"token_type_ids\": tokenized[\"token_type_ids\"].reshape((-1,)),\n",
    "        \"targets\": targets,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db4a9e",
   "metadata": {},
   "source": [
    "Finally, we can create the train and test dataset.\n",
    "Note that the test dataset does not need shuffling.\n",
    "Otherwise, these two datasets are simply performing sharding and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9444ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset():\n",
    "    train_dataset = train_data_pipe\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.sharding_filter()\n",
    "    train_dataset = train_dataset.map(unpack_row)\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def get_test_dataset():\n",
    "    test_dataset = test_data_pipe\n",
    "    test_dataset = test_dataset.sharding_filter()\n",
    "    test_dataset = test_dataset.map(unpack_row)\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d13bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_train_dataset()\n",
    "test_dataset = get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51488b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"115pt\" height=\"247pt\"\n",
       " viewBox=\"0.00 0.00 115.00 247.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 243)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-243 111,-243 111,4 -4,4\"/>\n",
       "<!-- Mapper&#45;373012000 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Mapper&#45;373012000</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"80.5,-184 26.5,-184 26.5,-165 80.5,-165 80.5,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">Mapper</text>\n",
       "</g>\n",
       "<!-- Shuffler&#45;377149987 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Shuffler&#45;377149987</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"86,-129 21,-129 21,-110 86,-110 86,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">Shuffler</text>\n",
       "</g>\n",
       "<!-- Mapper&#45;373012000&#45;&gt;Shuffler&#45;377149987 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Mapper&#45;373012000&#45;&gt;Shuffler&#45;377149987</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-164.75C53.5,-157.8 53.5,-147.85 53.5,-139.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-139.09 53.5,-129.09 50,-139.09 57,-139.09\"/>\n",
       "</g>\n",
       "<!-- ShardingFilter&#45;372785576 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>ShardingFilter&#45;372785576</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"104,-74 3,-74 3,-55 104,-55 104,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">ShardingFilter</text>\n",
       "</g>\n",
       "<!-- Shuffler&#45;377149987&#45;&gt;ShardingFilter&#45;372785576 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Shuffler&#45;377149987&#45;&gt;ShardingFilter&#45;372785576</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-109.75C53.5,-102.8 53.5,-92.85 53.5,-84.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-84.09 53.5,-74.09 50,-84.09 57,-84.09\"/>\n",
       "</g>\n",
       "<!-- Mapper&#45;377149930 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Mapper&#45;377149930</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"80.5,-19 26.5,-19 26.5,0 80.5,0 80.5,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\">Mapper</text>\n",
       "</g>\n",
       "<!-- ShardingFilter&#45;372785576&#45;&gt;Mapper&#45;377149930 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>ShardingFilter&#45;372785576&#45;&gt;Mapper&#45;377149930</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-54.75C53.5,-47.8 53.5,-37.85 53.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-29.09 53.5,-19.09 50,-29.09 57,-29.09\"/>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;373011901 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>IterableWrapper&#45;373011901</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107,-239 0,-239 0,-220 107,-220 107,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\">IterableWrapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;373011901&#45;&gt;Mapper&#45;373012000 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>IterableWrapper&#45;373011901&#45;&gt;Mapper&#45;373012000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-219.75C53.5,-212.8 53.5,-202.85 53.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-194.09 53.5,-184.09 50,-194.09 57,-194.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x167ada560>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdata.datapipes.utils.to_graph(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3eee79",
   "metadata": {},
   "source": [
    "Now, we can build our model.\n",
    "We'll simply take the pretrained model and add a linear layer after pooling.\n",
    "Note that BERT has a feature dimension of 768. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f2fac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.base = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.fc = torch.nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, x = self.base(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False,\n",
    "        )\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbcd6d",
   "metadata": {},
   "source": [
    "We're almost there!\n",
    "Let's defined how to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f95fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader):\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.perf_counter()\n",
    "    for iteration, data in enumerate(dataloader, 1):\n",
    "        input_ids = data[\"input_ids\"].to(device, dtype=torch.long, non_blocking=True)\n",
    "        attention_mask = data[\"attention_mask\"].to(\n",
    "            device, dtype=torch.long, non_blocking=True\n",
    "        )\n",
    "        token_type_ids = data[\"token_type_ids\"].to(\n",
    "            device, dtype=torch.long, non_blocking=True\n",
    "        )\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += len(outputs) * loss.item()\n",
    "        total_samples += len(outputs)\n",
    "        if iteration % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                mean_loss = total_loss / total_samples\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "                samples_per_second = total_samples / elapsed_time\n",
    "                progress = total_samples / NUM_TRAIN_SAMPLES\n",
    "                msg = (\n",
    "                    f\"[Train {progress:.1%}]: loss: {mean_loss:.2f} \"\n",
    "                    f\"({samples_per_second:.2f} samples/sec)\"\n",
    "                )\n",
    "                print(msg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = total_loss / total_samples\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        start_time = time.perf_counter()\n",
    "        for iteration, data in enumerate(dataloader, 1):\n",
    "            input_ids = data[\"input_ids\"].to(\n",
    "                device, dtype=torch.long, non_blocking=True\n",
    "            )\n",
    "            attention_mask = data[\"attention_mask\"].to(\n",
    "                device, dtype=torch.long, non_blocking=True\n",
    "            )\n",
    "            token_type_ids = data[\"token_type_ids\"].to(\n",
    "                device, dtype=torch.long, non_blocking=True\n",
    "            )\n",
    "            targets = data[\"targets\"].to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += len(outputs) * loss.item()\n",
    "            total_samples += len(outputs)\n",
    "            preds = torch.argmax(outputs, axis=1)\n",
    "            total_correct += torch.sum((preds == targets).float()).item()\n",
    "            if iteration % 100 == 0:\n",
    "                mean_loss = total_loss / total_samples\n",
    "                accuracy = total_correct / total_samples\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "                samples_per_second = total_samples / elapsed_time\n",
    "                progress = total_samples / NUM_TEST_SAMPLES\n",
    "                msg = (\n",
    "                    f\"[Test {progress:.1%}]: loss: {mean_loss:.2f}, \"\n",
    "                    f\"accuracy: {accuracy:.1%} ({samples_per_second:.2f} samples/sec)\"\n",
    "                )\n",
    "                print(msg)\n",
    "        mean_loss = total_loss / total_samples\n",
    "        accuracy = total_correct / total_samples\n",
    "        return mean_loss, accuracy\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ade59",
   "metadata": {},
   "source": [
    "Now we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd949981",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "model = BERTClassifier(len(classes_to_int))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d08ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train 6.3%]: loss: 5.15 (223.96 samples/sec)\n",
      "[Train 12.6%]: loss: 4.82 (237.17 samples/sec)\n",
      "[Train 18.9%]: loss: 4.52 (242.71 samples/sec)\n",
      "[Train 25.2%]: loss: 4.27 (245.09 samples/sec)\n",
      "[Train 31.5%]: loss: 4.05 (247.04 samples/sec)\n",
      "[Train 37.8%]: loss: 3.85 (248.59 samples/sec)\n",
      "[Train 44.1%]: loss: 3.67 (249.50 samples/sec)\n",
      "[Train 50.3%]: loss: 3.51 (250.61 samples/sec)\n",
      "[Train 56.6%]: loss: 3.37 (251.58 samples/sec)\n",
      "[Train 62.9%]: loss: 3.24 (251.81 samples/sec)\n",
      "[Train 69.2%]: loss: 3.12 (251.84 samples/sec)\n",
      "[Train 75.5%]: loss: 3.02 (251.83 samples/sec)\n",
      "[Train 81.8%]: loss: 2.92 (251.89 samples/sec)\n",
      "[Train 88.1%]: loss: 2.83 (251.96 samples/sec)\n",
      "[Train 94.4%]: loss: 2.74 (252.09 samples/sec)\n",
      "[Test 12.4%]: loss: 1.49, accuracy: 62.5% (696.88 samples/sec)\n",
      "[Test 24.8%]: loss: 1.48, accuracy: 62.2% (779.03 samples/sec)\n",
      "[Test 37.2%]: loss: 1.47, accuracy: 62.4% (811.89 samples/sec)\n",
      "[Test 49.6%]: loss: 1.44, accuracy: 63.1% (826.75 samples/sec)\n",
      "[Test 62.0%]: loss: 1.42, accuracy: 63.4% (836.01 samples/sec)\n",
      "[Test 74.4%]: loss: 1.41, accuracy: 63.7% (844.06 samples/sec)\n",
      "[Test 86.8%]: loss: 1.41, accuracy: 63.8% (849.12 samples/sec)\n",
      "[Test 99.2%]: loss: 1.40, accuracy: 64.1% (853.80 samples/sec)\n",
      "Epoch 0 train_loss: 2.67, test_loss: 1.40, test_accuracy: 64.1%\n",
      "[Train 6.3%]: loss: 1.57 (221.54 samples/sec)\n",
      "[Train 12.6%]: loss: 1.52 (235.51 samples/sec)\n",
      "[Train 18.9%]: loss: 1.46 (241.57 samples/sec)\n",
      "[Train 25.2%]: loss: 1.42 (244.72 samples/sec)\n",
      "[Train 31.5%]: loss: 1.39 (246.64 samples/sec)\n",
      "[Train 37.8%]: loss: 1.35 (247.98 samples/sec)\n",
      "[Train 44.1%]: loss: 1.32 (249.21 samples/sec)\n",
      "[Train 50.3%]: loss: 1.29 (249.86 samples/sec)\n",
      "[Train 56.6%]: loss: 1.27 (250.33 samples/sec)\n",
      "[Train 62.9%]: loss: 1.25 (250.72 samples/sec)\n",
      "[Train 69.2%]: loss: 1.23 (251.02 samples/sec)\n",
      "[Train 75.5%]: loss: 1.21 (251.22 samples/sec)\n",
      "[Train 81.8%]: loss: 1.19 (251.41 samples/sec)\n",
      "[Train 88.1%]: loss: 1.17 (251.57 samples/sec)\n",
      "[Train 94.4%]: loss: 1.16 (251.60 samples/sec)\n",
      "[Test 12.4%]: loss: 0.99, accuracy: 71.1% (700.25 samples/sec)\n",
      "[Test 24.8%]: loss: 0.98, accuracy: 71.2% (780.19 samples/sec)\n",
      "[Test 37.2%]: loss: 0.97, accuracy: 71.5% (812.84 samples/sec)\n",
      "[Test 49.6%]: loss: 0.94, accuracy: 72.2% (827.75 samples/sec)\n",
      "[Test 62.0%]: loss: 0.92, accuracy: 72.7% (836.83 samples/sec)\n",
      "[Test 74.4%]: loss: 0.90, accuracy: 73.0% (844.54 samples/sec)\n",
      "[Test 86.8%]: loss: 0.90, accuracy: 73.1% (849.09 samples/sec)\n",
      "[Test 99.2%]: loss: 0.89, accuracy: 73.3% (853.72 samples/sec)\n",
      "Epoch 1 train_loss: 1.14, test_loss: 0.89, test_accuracy: 73.4%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=NUM_TRAIN_WORKERS,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    num_workers=NUM_TEST_WORKERS,\n",
    ")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    test_loss, test_accuracy = test(model, test_dataloader)\n",
    "    print(\n",
    "        f\"Epoch {epoch} train_loss: {train_loss:.2f}, \"\n",
    "        f\"test_loss: {test_loss:.2f}, test_accuracy: {test_accuracy:.1%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9b11a",
   "metadata": {},
   "source": [
    "## Trying It Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b052160",
   "metadata": {},
   "source": [
    "Now that we have a model, we can see how it performs on some (perhaps) realistic examples üòÅ.\n",
    "Below, we pick a few translations of \"Croissants are tasty!\" to see how the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da8c4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cases = [\n",
    "    \"Croissants are tasty!\",\n",
    "    \"¬°Los croissants son deliciosos!\",\n",
    "    \"Les croissants sont savoureux!\",\n",
    "    \"ÁæäËßíÈù¢ÂåÖÂæàÂ•ΩÂêÉ!\",\n",
    "    \"◊î◊ß◊®◊ï◊ê◊°◊ï◊†◊ô◊ù ◊ò◊¢◊ô◊û◊ô◊ù!\",\n",
    "    \"Kruvasanlar juda mazali!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2859ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Croissants are tasty!' ‚Üí 'eng_Latn'\n",
      "'¬°Los croissants son deliciosos!' ‚Üí 'ast_Latn'\n",
      "'Les croissants sont savoureux!' ‚Üí 'fra_Latn'\n",
      "'ÁæäËßíÈù¢ÂåÖÂæàÂ•ΩÂêÉ!' ‚Üí 'zho_Hant'\n",
      "'◊î◊ß◊®◊ï◊ê◊°◊ï◊†◊ô◊ù ◊ò◊¢◊ô◊û◊ô◊ù!' ‚Üí 'heb_Hebr'\n",
      "'Kruvasanlar juda mazali!' ‚Üí 'uzn_Latn'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "for case in sample_cases:\n",
    "    toks = tokenizer(case, return_tensors=\"pt\")\n",
    "    logits = model(\n",
    "        toks[\"input_ids\"].to(device),\n",
    "        toks[\"attention_mask\"].to(device),\n",
    "        toks[\"token_type_ids\"].to(device),\n",
    "    )\n",
    "    y_pred = torch.argmax(logits, axis=1).item()\n",
    "    pred_lang = int_to_classes[y_pred]\n",
    "    print(f\"'{case}' ‚Üí '{pred_lang}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
